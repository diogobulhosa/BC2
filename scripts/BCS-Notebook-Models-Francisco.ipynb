{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import precision_score, classification_report, confusion_matrix, f1_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# smote\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv(r'../data/Train_year.csv', index_col=0)\n",
    "y_Train = pd.read_csv(r'../data/y_Train_year.csv', index_col=0)\n",
    "\n",
    "Test = pd.read_csv(r'../data/Test_year.csv', index_col=0)\n",
    "y_Test = pd.read_csv(r'../data/y_Test_year.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ADR', 'Adults', 'ArrivalDateWeekNumber', 'BookingChanges',\n",
    "       'LeadTime',\n",
    "        'StaysInWeekNights',\n",
    "       'PreviousCancellationRate',\n",
    "       'TotalOfSpecialRequests','x2_avg_booker',\n",
    "       'x2_good_booker', 'x2_low_booker', 'x2_no_booker', 'x2_super_booker',\n",
    "       'x4_Low_Season']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Test Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(X_train, X_val, y_train, pred_train, y_val, pred_val, model):\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                     TRAIN                                                 ')\n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_train, pred_train))\n",
    "    print(confusion_matrix(y_train, pred_train))\n",
    "    print(\"Score: \" + str(model.score(X_train, y_train)))\n",
    "    print(\"F1 Score: \" + str(f1_score(y_train, pred_train)))\n",
    "\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                VALIDATION                                                 ')\n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_val, pred_val))\n",
    "    print(confusion_matrix(y_val, pred_val))\n",
    "    print(\"Score: \" + str(model.score(X_val, y_val)))\n",
    "    print(\"F1 Score: \" + str(f1_score(y_val, pred_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score(model, data_to_slice, y_to_slice, columns_to_use, smote=True):\n",
    "    # apply kfold\n",
    "    skf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5)\n",
    "    # create lists to store the results from the different models\n",
    "    score_train = []\n",
    "    score_test = []\n",
    "    f1_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    tn_avg = 0\n",
    "    fp_avg = 0\n",
    "    fn_avg = 0\n",
    "    tp_avg = 0\n",
    "    count = 0\n",
    "    for train_index, test_index in skf.split(data_to_slice[columns_to_use], y_to_slice):\n",
    "        # get the indexes of the observations assigned for each partition\n",
    "        X_train, X_val = data_to_slice[columns_to_use].iloc[train_index], data_to_slice[columns_to_use].iloc[test_index]\n",
    "        y_train, y_val = y_to_slice.iloc[train_index], y_to_slice.iloc[test_index]\n",
    "\n",
    "        # SMOTE Ã‰ AQUI\n",
    "        if smote:\n",
    "\n",
    "            smote = SMOTE(random_state=11)\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # applies the model\n",
    "        model_fit = model.fit(X_train, y_train)\n",
    "        # predicts training\n",
    "        y_pred_train = model_fit.predict(X_train)\n",
    "        # predicts validation\n",
    "        y_pred_val = model_fit.predict(X_val)\n",
    "        # prints metric results\n",
    "\n",
    "        #metrics(X_train, X_val, y_train, y_pred_train, y_val, y_pred_val, model)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, y_pred_val).ravel()\n",
    "        count += 1\n",
    "        tn_avg += tn\n",
    "        fp_avg += fp\n",
    "        fn_avg += fn\n",
    "        tp_avg += tp\n",
    "\n",
    "        value_train = model.score(X_train, y_train)\n",
    "        # check the mean accuracy for the test\n",
    "        value_test = model.score(X_val, y_val)\n",
    "        f1_score_val = f1_score(y_val, y_pred_val)\n",
    "        precision_val = precision_score(y_val, y_pred_val)\n",
    "        recall_val = recall_score(y_val, y_pred_val)\n",
    "        # append the accuracies, the time and the number of iterations in the corresponding list\n",
    "        score_train.append(value_train)\n",
    "        score_test.append(value_test)\n",
    "        f1_list.append(f1_score_val)\n",
    "        precision_list.append(precision_val)\n",
    "        recall_list.append(recall_val)\n",
    "\n",
    "    avg_train = round(np.mean(score_train), 3)\n",
    "    avg_test = round(np.mean(score_test), 3)\n",
    "    std_train = round(np.std(score_train), 2)\n",
    "    std_test = round(np.std(score_test), 2)\n",
    "    avg_f1 = round(np.mean(f1_list), 3)\n",
    "    std_f1 = round(np.std(f1_list), 2)\n",
    "    avg_precision = round(np.mean(precision_list), 3)\n",
    "    std_precision = round(np.std(precision_list), 2)\n",
    "    avg_recall = round(np.mean(recall_list), 3)\n",
    "    std_recall = round(np.mean(recall_list), 2)\n",
    "\n",
    "    tn_avg = tn_avg / count\n",
    "    fp_avg = fp_avg / count\n",
    "    fn_avg = fn_avg / count\n",
    "    tp_avg = tp_avg / count\n",
    "    #print(confusion_matrix(y_val, y_pred_val))\n",
    "    print(str(tp_avg) + ' , ' + str(fp_avg) +\n",
    "          '\\n' + str(fn_avg) + ' , ' + str(tn_avg))\n",
    "    return str(avg_train) + '+/-' + str(std_train),\\\n",
    "        str(avg_test) + '+/-' + str(std_test),\\\n",
    "        'F1 SCORE : ' + str(avg_f1) + '+/-' + str(std_f1), \\\n",
    "        'Precision : ' + str(avg_precision) + '+/-' + str(std_precision), \\\n",
    "        'RECALL :' + str(avg_recall) + '+/-' + str(std_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_models(model, Train, y_Train, Test, y_Test):\n",
    "    model_fit = model.fit(Train, y_Train)\n",
    "    y_pred_test = model_fit.predict(Test)\n",
    "    a = 0\n",
    "    ones = 0\n",
    "    for i, x in enumerate(y_Test.IsCanceled):\n",
    "        #print(i, y_pred_test[i],x)\n",
    "        if y_pred_test[i] == x:\n",
    "            a += 1\n",
    "            if x == 1:\n",
    "                ones += 1\n",
    "\n",
    "    print(classification_report(y_Test, y_pred_test))\n",
    "    print(confusion_matrix(y_Test, y_pred_test))\n",
    "    print(a)\n",
    "    print(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.legend_handler import HandlerLine2D\n",
    "# n_estimators = np.arange(1, 50)\n",
    "# train_results = []\n",
    "# test_results = []\n",
    "# diff=[]\n",
    "\n",
    "# for estimator in n_estimators:\n",
    "#     rf = RandomForestClassifier(n_estimators=estimator, criterion= 'gini')\n",
    "#     rf.fit(Train, y_Train)\n",
    "#     train_pred = rf.predict(Train)\n",
    "#     train_score = f1_score(y_Train,train_pred)\n",
    "#     train_results.append(train_score)\n",
    "\n",
    "#     test_pred = rf.predict(Test)\n",
    "#     val_score = f1_score(y_Test,test_pred)\n",
    "#     test_results.append(val_score)\n",
    "#     diff.append(val_score-train_score)\n",
    "\n",
    "# line1, = plt.plot(n_estimators, train_results, 'b', label='Train F1')\n",
    "# line2, = plt.plot(n_estimators, test_results, 'r', label='Test F1')\n",
    "# plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.xlabel('n_estimators')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_estimator=pd.DataFrame([pd.Series(n_estimators,name='n_estimators',dtype=int),pd.Series(train_results,name='Train'),pd.Series(test_results,name='Val'),pd.Series(diff,name='Diff')]).T\n",
    "# df_estimator.sort_values(by='Val',ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gini = RandomForestClassifier(criterion='gini', n_estimators=25)\n",
    "rf_entropy = RandomForestClassifier(criterion='entropy', n_estimators=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119.96 , 530.24\n",
      "715.24 , 4254.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.993+/-0.0',\n",
       " '0.812+/-0.0',\n",
       " 'F1 SCORE : 0.643+/-0.01',\n",
       " 'Precision : 0.679+/-0.01',\n",
       " 'RECALL :0.61+/-0.61')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score(rf_gini, Train, y_Train, Train.columns, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118.96 , 523.92\n",
      "716.24 , 4261.28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.993+/-0.0',\n",
       " '0.813+/-0.0',\n",
       " 'F1 SCORE : 0.643+/-0.01',\n",
       " 'Precision : 0.681+/-0.01',\n",
       " 'RECALL :0.61+/-0.61')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score(rf_entropy, Train, y_Train, Train.columns, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf_entropy.predict(Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14174"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0\n",
    "for i, x in enumerate(y_Test.IsCanceled):\n",
    "    if prediction[i] == x:\n",
    "        a += 1\n",
    "a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy looks like the best choice although both criterias are very similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # forest params\n",
    "# param_grid = {\n",
    "#     'max_depth' : range(12,20),\n",
    "#     'n_estimators' : range(25,27),\n",
    "#     'min_impurity_decrease' : np.linspace(0.0000001,0.01,100),\n",
    "#     'min_samples_leaf': range(110,140,10),\n",
    "#     'min_samples_split':range(50,100,10)\n",
    "# }\n",
    "\n",
    "# # Instantiate the grid search model\n",
    "# grid_search = GridSearchCV(estimator = rf_entropy, param_grid = param_grid, scoring = 'f1')\n",
    "\n",
    "# grid_search.fit(Train, y_Train)\n",
    "# grid_search.best_params_, grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135.28 , 644.4\n",
      "699.92 , 4140.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.793+/-0.0',\n",
       " '0.797+/-0.0',\n",
       " 'F1 SCORE : 0.628+/-0.01',\n",
       " 'Precision : 0.638+/-0.01',\n",
       " 'RECALL :0.619+/-0.62')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(criterion='entropy', max_depth=15, min_impurity_decrease=0.00010110909090909092,\n",
    "                                  min_samples_leaf=120, min_samples_split=50, n_estimators=130)\n",
    "avg_score(rf_model, Train, y_Train, Train.columns, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935.92 , 261.88\n",
      "899.28 , 4523.32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.853+/-0.0',\n",
       " '0.825+/-0.0',\n",
       " 'F1 SCORE : 0.617+/-0.01',\n",
       " 'Precision : 0.781+/-0.01',\n",
       " 'RECALL :0.51+/-0.51')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model2 = RandomForestClassifier(\n",
    "    n_estimators=130, max_depth=15, random_state=10)\n",
    "avg_score(rf_model2, Train, y_Train, Train.columns, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.90      0.81     13183\n",
      "           1       0.66      0.38      0.48      6826\n",
      "\n",
      "    accuracy                           0.72     20009\n",
      "   macro avg       0.70      0.64      0.65     20009\n",
      "weighted avg       0.71      0.72      0.70     20009\n",
      "\n",
      "[[11871  1312]\n",
      " [ 4244  2582]]\n",
      "14453\n",
      "2582\n"
     ]
    }
   ],
   "source": [
    "Test_models(rf_model, Train, y_Train, Test, y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.89      0.81     13183\n",
      "           1       0.66      0.41      0.51      6826\n",
      "\n",
      "    accuracy                           0.73     20009\n",
      "   macro avg       0.70      0.65      0.66     20009\n",
      "weighted avg       0.72      0.73      0.71     20009\n",
      "\n",
      "[[11741  1442]\n",
      " [ 3998  2828]]\n",
      "14569\n",
      "2828\n",
      "0.6622950819672131\n",
      "0.4142982713155582\n",
      "0.5097332372025954\n"
     ]
    }
   ],
   "source": [
    "Test_models(rf_model2, Train, y_Train, Test, y_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(random_state = 10 , n_estimators = 130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:08:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:09:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:09:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:09:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:09:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:09:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'max_depth': 3}, 0.12671877512707025)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forest params\n",
    "param_grid = {\n",
    "    'max_depth' : range(3,8,1),\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'f1')\n",
    "\n",
    "grid_search.fit(Train, y_Train)\n",
    "grid_search.best_params_, grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(verbosity = 0, n_estimators = 163, max_depth = 5, random_state = 10, learning_rate = 0.2)\n",
    "# avg_score(model, Train, y_Train, Train.columns, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.86      0.80     13183\n",
      "           1       0.63      0.47      0.54      6826\n",
      "\n",
      "    accuracy                           0.72     20009\n",
      "   macro avg       0.69      0.66      0.67     20009\n",
      "weighted avg       0.71      0.72      0.71     20009\n",
      "\n",
      "[[11280  1903]\n",
      " [ 3627  3199]]\n",
      "14479\n",
      "3199\n"
     ]
    }
   ],
   "source": [
    "Test_models(model, Train, y_Train, Test, y_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussianNB = GaussianNB(var_smoothing= 0.003511191734215131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'var_smoothing': 0.003511191734215131}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_space = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "    \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(GaussianNB, parameter_space, scoring = 'f1')\n",
    "\n",
    "grid_search.fit(Train[features], y_Train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1453.44 , 2252.4\n",
      "381.76 , 2532.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.668+/-0.01',\n",
       " '0.602+/-0.01',\n",
       " 'F1 SCORE : 0.525+/-0.01',\n",
       " 'Precision : 0.392+/-0.01',\n",
       " 'RECALL :0.792+/-0.79')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score(GaussianNB, Train, y_Train, Train.columns, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.59      0.68     13183\n",
      "           1       0.47      0.71      0.57      6826\n",
      "\n",
      "    accuracy                           0.63     20009\n",
      "   macro avg       0.63      0.65      0.62     20009\n",
      "weighted avg       0.69      0.63      0.64     20009\n",
      "\n",
      "[[7780 5403]\n",
      " [1989 4837]]\n",
      "12617\n",
      "4837\n"
     ]
    }
   ],
   "source": [
    "Test_models(GaussianNB, Train, y_Train, Test, y_Test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c5bc60f1b709cc28d984b044e46a36e86d71fb0a6f2e3589923fa1d0135e80a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# models \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,f1_score, recall_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "#smote \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv(r'../data/Train_year.csv', index_col=0)\n",
    "y_Train = pd.read_csv(r'../data/y_Train_year.csv', index_col= 0)\n",
    "\n",
    "Test = pd.read_csv(r'../data/Test_year.csv', index_col= 0)\n",
    "y_Test = pd.read_csv(r'../data/y_Test_year.csv', index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ADR', 'LeadTime','StaysInWeekNights', 'TotalOfSpecialRequests',\n",
    "        'BookingChanges', 'PreviousBookingsNotCanceled', 'RequiredCarParkingSpaces', 'PreviousCancellations',\n",
    "        'x0_BB', 'x0_SC', 'x1_A', 'x1_B', 'x1_D',\n",
    "       'x1_E', 'x1_F', 'x1_G', 'x2_avg_booker', 'x2_good_booker',\n",
    "       'x2_low_booker', 'x2_no_booker', 'x2_super_booker', 'x3_Autumn',\n",
    "       'x3_Spring', 'x3_Summer', 'x4_Low_Season']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Test Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(X_train, X_val, y_train, pred_train , y_val, pred_val, model):\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                     TRAIN                                                 ')\n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_train, pred_train))\n",
    "    print(confusion_matrix(y_train, pred_train))\n",
    "    print(\"Score: \"+ str(model.score(X_train, y_train)))\n",
    "    print(\"F1 Score: \"+ str(f1_score(y_train, pred_train)))\n",
    "    print(\"Recall: \"+ str(recall_score(y_train, pred_train)))\n",
    "    \n",
    "\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                VALIDATION                                                 ')\n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_val, pred_val))\n",
    "    print(confusion_matrix(y_val, pred_val))\n",
    "    print(\"Score: \"+ str(model.score(X_val, y_val)))\n",
    "    print(\"F1 Score: \"+ str(f1_score(y_val, pred_val)))\n",
    "    print(\"Recall: \"+ str(recall_score(y_val, pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score(model, data_to_slice, y_to_slice, columns_to_use, smote = True):\n",
    "    # apply kfold\n",
    "    skf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5)\n",
    "    # create lists to store the results from the different models \n",
    "    score_train = []\n",
    "    score_test = []\n",
    "    f1_list = []\n",
    "    recall_list =[]\n",
    "    tn_avg = 0\n",
    "    fp_avg = 0\n",
    "    fn_avg = 0\n",
    "    tp_avg = 0\n",
    "    count = 0\n",
    "    for train_index, test_index in skf.split(data_to_slice[columns_to_use],y_to_slice):\n",
    "        # get the indexes of the observations assigned for each partition\n",
    "        X_train, X_val = data_to_slice[columns_to_use].iloc[train_index], data_to_slice[columns_to_use].iloc[test_index]\n",
    "        y_train, y_val = y_to_slice.iloc[train_index], y_to_slice.iloc[test_index]\n",
    "        \n",
    "        # SMOTE Ã‰ AQUI \n",
    "        if smote:\n",
    "             \n",
    "            smote = SMOTE(random_state = 11)\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # applies the model \n",
    "        model_fit = model.fit(X_train, y_train)\n",
    "        # predicts training \n",
    "        y_pred_train =  model_fit.predict(X_train)\n",
    "        #predicts validation \n",
    "        y_pred_val = model_fit.predict(X_val)\n",
    "        # prints metric results \n",
    "        \n",
    "        #metrics(X_train, X_val, y_train, y_pred_train, y_val, y_pred_val, model)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, y_pred_val).ravel()\n",
    "        count += 1\n",
    "        tn_avg += tn\n",
    "        fp_avg += fp\n",
    "        fn_avg += fn\n",
    "        tp_avg += tp\n",
    "\n",
    "        \n",
    "        value_train = model.score(X_train, y_train)\n",
    "        # check the mean accuracy for the test\n",
    "        value_test = model.score(X_val,y_val)\n",
    "        f1_score_val = f1_score(y_val, y_pred_val)\n",
    "        recall_val = recall_score(y_val, y_pred_val)\n",
    "        # append the accuracies, the time and the number of iterations in the corresponding list\n",
    "        score_train.append(value_train)\n",
    "        score_test.append(value_test)\n",
    "        f1_list.append(f1_score_val)\n",
    "        recall_list.append(recall_val)\n",
    "  \n",
    "    avg_train = round(np.mean(score_train),3)\n",
    "    avg_test = round(np.mean(score_test),3)\n",
    "    std_train = round(np.std(score_train),2)\n",
    "    std_test = round(np.std(score_test),2)\n",
    "    avg_f1 = round(np.mean(f1_list),3)\n",
    "    std_f1 = round(np.std(f1_list),2)\n",
    "    avg_recall = round(np.mean(recall_list),3)\n",
    "    std_recall = round(np.std(recall_list),2)\n",
    "\n",
    "    tn_avg = tn_avg / count\n",
    "    fp_avg = fp_avg / count\n",
    "    fn_avg = fn_avg / count\n",
    "    tp_avg = tp_avg / count\n",
    "    #print(confusion_matrix(y_val, y_pred_val))\n",
    "    print(str(tp_avg)+ ' , ' + str(fp_avg) + '\\n' + str(fn_avg) + ' , ' +  str(tn_avg))\n",
    "    return str(avg_train) + '+/-' + str(std_train),\\\n",
    "            str(avg_test) + '+/-' + str(std_test) , str(avg_f1) + '+/-' + str(std_f1), avg_f1,\\\n",
    "                ' Recall: ' + str(avg_recall) + '+/-' + str(std_recall), avg_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285.76 , 1292.72\n",
      "549.44 , 3492.48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.717+/-0.0',\n",
       " '0.722+/-0.01',\n",
       " '0.583+/-0.01',\n",
       " 0.583,\n",
       " ' Recall: 0.701+/-0.01',\n",
       " 0.701)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg = LogisticRegression(random_state=11)\n",
    "\n",
    "avg_score(LogReg, Train[features], y_Train,Train[features].columns, smote = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2,\n",
       " 'dual': True,\n",
       " 'max_iter': 200,\n",
       " 'multi_class': 'auto',\n",
       " 'penalty': 'l2',\n",
       " 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['l1','l2','elasticnet','none'],\n",
    "    'dual':[True, False],\n",
    "    'C':[0.05, 0.2, 0.5, 1, 2],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [50,100,200,500],\n",
    "    'multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "    \n",
    "}\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = LogReg, param_grid = param_grid, scoring = 'f1')\n",
    "\n",
    "grid_search.fit(Train, y_Train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1284.64 , 1290.84\n",
      "550.56 , 3494.36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.718+/-0.0',\n",
       " '0.722+/-0.01',\n",
       " '0.582+/-0.01',\n",
       " 0.582,\n",
       " ' Recall: 0.7+/-0.01',\n",
       " 0.7)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg = LogisticRegression(random_state=11,C= 2,dual= True,max_iter= 200,\n",
    " multi_class= 'auto',penalty= 'l2', solver= 'liblinear' )\n",
    "\n",
    "avg_score(LogReg, Train[features], y_Train,Train[features].columns, smote = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies the model \n",
    "model_fit = LogReg.fit(Train, y_Train)\n",
    "# predicts test \n",
    "y_pred_test =  model_fit.predict(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20009\n",
      "36\n",
      "20009\n",
      "IsCanceled    6826\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred_test))\n",
    "print(y_pred_test.sum())\n",
    "\n",
    "print(len(y_Test))\n",
    "print(y_Test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13211"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0\n",
    "for i,x in enumerate(y_Test.IsCanceled):\n",
    "    if y_pred_test[i] == x:\n",
    "        a+=1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1271.24 , 1249.76\n",
      "563.96 , 3535.44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.874+/-0.0',\n",
       " '0.726+/-0.01',\n",
       " '0.584+/-0.01',\n",
       " 0.584,\n",
       " ' Recall: 0.693+/-0.01',\n",
       " 0.693)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelKNN = KNeighborsClassifier()\n",
    "\n",
    "avg_score(modelKNN, Train, y_Train,Train.columns, smote = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 5, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [5,10,50,100],\n",
    "    'weights':['uniform', 'distance'],\n",
    "    'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10,30,50]\n",
    "}\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = modelKNN, param_grid = param_grid, scoring = 'f1')\n",
    "\n",
    "grid_search.fit(Train, y_Train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1239.8 , 1140.08\n",
      "595.4 , 3645.12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.871+/-0.0',\n",
       " '0.738+/-0.0',\n",
       " '0.588+/-0.01',\n",
       " 0.588,\n",
       " ' Recall: 0.676+/-0.01',\n",
       " 0.676)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelKNN = KNeighborsClassifier(n_neighbors=5, weights='uniform',algorithm='auto',\n",
    "leaf_size= 10)\n",
    "\n",
    "avg_score(modelKNN, Train[features], y_Train,features, smote = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies the model \n",
    "model_fit = modelKNN.fit(Train, y_Train)\n",
    "# predicts test \n",
    "y_pred_test =  model_fit.predict(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20009\n",
      "4851\n",
      "20009\n",
      "IsCanceled    6826\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred_test))\n",
    "print(y_pred_test.sum())\n",
    "\n",
    "print(len(y_Test))\n",
    "print(y_Test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13840"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0\n",
    "for i,x in enumerate(y_Test.IsCanceled):\n",
    "    if y_pred_test[i] == x:\n",
    "        a+=1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c5bc60f1b709cc28d984b044e46a36e86d71fb0a6f2e3589923fa1d0135e80a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
